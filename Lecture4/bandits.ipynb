{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the bandit environment with Bernoulli rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BernoulliBanditEnv(object):\n",
    "    def __init__(self, num_arms=10, p=None):\n",
    "        self.num_arms = num_arms\n",
    "        self.actions = np.arange(num_arms)\n",
    "\n",
    "        # probability of reward for each arm\n",
    "        if p==None:\n",
    "            self.p = np.random.beta(0.5, 0.5, size=num_arms)\n",
    "        elif len(p) == num_arms:\n",
    "            self.p = p\n",
    "        else:\n",
    "            warning()\n",
    "        self.best_action = np.argmax(self.p)\n",
    "\n",
    "    def reward(self, action):\n",
    "        return np.random.binomial(1, p=self.p[action])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define an agent class that contains several learning and decision rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, learning_rule, decision_rule, param=None):\n",
    "        self.decision_rule = decision_rule\n",
    "        self.learning_rule = learning_rule\n",
    "        self.iteration = 0\n",
    "        # organize parameters of learning rules in what follows\n",
    "        if decision_rule == \"epsilon-greedy\":\n",
    "            self.epsilon = param[\"epsilon\"]\n",
    "            \n",
    "        if decision_rule == \"UCB\":\n",
    "            self.UCB_param = param[\"UCB_param\"]\n",
    "                    \n",
    "        if learning_rule == \"RW\":\n",
    "            self.learning_rate = param[\"learning_rate\"]\n",
    "            \n",
    "    def environment(self, env, init_q):\n",
    "        self.env = env\n",
    "        self.k = env.num_arms\n",
    "        self.actions = np.arange(self.k)\n",
    "        self.act_count = np.zeros(self.k)\n",
    "        if self.learning_rule == \"BayesianBetaPrior\":\n",
    "            self.alpha = np.random.uniform(size=self.k)\n",
    "            self.beta = np.random.uniform(size=self.k)\n",
    "        if not (len(init_q) == self.k):\n",
    "            raise Exception('Number of initial values ({}) does not correspond to number of arms ({}).'.format(len(init_q), self.k))\n",
    "        else:\n",
    "            self.q_estimate = init_q\n",
    "\n",
    "    def learn(self, a, r):\n",
    "        if self.learning_rule == \"averaging\":\n",
    "            self.q_estimate[a] += 1/self.act_count[a] * (r - self.q_estimate[a])\n",
    "\n",
    "        if self.learning_rule == \"RW\":\n",
    "            self.act_count[a] += 1\n",
    "            self.q_estimate[a] += self.learning_rate * (r - self.q_estimate[a])\n",
    "            \n",
    "        if self.learning_rule == \"BayesianBetaPrior\":\n",
    "            # Bayesian update of Beta distribution\n",
    "            self.act_count[a] += 1\n",
    "            self.alpha[a] += r\n",
    "            self.beta[a] += 1 - r    \n",
    "            \n",
    "    def act(self):\n",
    "        self.iteration += 1\n",
    "        if self.decision_rule == \"greedy\":\n",
    "            q_best = np.max(self.q_estimate)\n",
    "            selected_action = np.random.choice([action for action, q in enumerate(self.q_estimate) if q == q_best])\n",
    "\n",
    "        if self.decision_rule == \"epsilon-greedy\":\n",
    "            if np.random.rand() < self.epsilon:   # with probability epsilon select a random arm\n",
    "                selected_action = np.random.choice(self.actions)\n",
    "            else:                                 # else apply the greedy choice\n",
    "                selected_action = np.argmax(self.q_estimate)\n",
    "\n",
    "        if self.decision_rule == \"UCB\":\n",
    "            UCB_objective = self.q_estimate + self.UCB_param * \\\n",
    "                np.sqrt(np.log(self.iteration) / (self.act_count + np.finfo(float).eps))  # add small number (machine precision) to avoid divergence at zero\n",
    "            q_best = np.max(UCB_objective)    # there are possibly multiple arms with the same Q value, pick at random among those:\n",
    "            selected_action = np.random.choice([action for action, q in enumerate(UCB_objective) if q == q_best])\n",
    "\n",
    "        if self.decision_rule == \"Thompson\":\n",
    "            # sample q-values from the Beta distribution\n",
    "            selected_action = np.argmax(np.random.beta(self.alpha, self.beta))\n",
    "        \n",
    "        self.act_count[selected_action] += 1\n",
    "\n",
    "        return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulateBandits(agents, narms, initp=None, initq=None, runs=2000, N=100):\n",
    "    rewards = np.zeros((len(agents), runs, N))\n",
    "    bestarm = np.zeros((len(agents), runs, N))\n",
    "    for i, agent in enumerate(agents):\n",
    "        for j in np.arange(runs):\n",
    "            environment = BernoulliBanditEnv(num_arms=narms, p=initp)\n",
    "            agent.environment(environment, initq if not(initq == None) else np.zeros(narms))\n",
    "            for n in np.arange(N):\n",
    "                a = agent.act()\n",
    "                r = environment.reward(a)\n",
    "                agent.learn(a, r)\n",
    "                rewards[i, j, n] = r\n",
    "                bestarm[i, j, n] = 1 if a == environment.best_action else 0    # did the agent choose the best action?\n",
    "    \n",
    "    return np.squeeze(np.mean(rewards, axis=1)), np.squeeze(np.mean(bestarm, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run simulations\n",
    "arms = 2\n",
    "epsilons = [0.1, 0.9]\n",
    "agents = []\n",
    "# initialize agents\n",
    "agents.append(Agent(\"averaging\", \"UCB\", {\"UCB_param\":0.5}))\n",
    "agents.append(Agent(\"BayesianBetaPrior\", \"Thompson\",{\"alpha\":0.5, \"beta\":0.3}))\n",
    "for eps in epsilons:\n",
    "    agents.append(Agent(\"averaging\", \"epsilon-greedy\", {\"epsilon\":eps}))\n",
    "agents.append(Agent(\"averaging\", \"greedy\"))\n",
    "\n",
    "start = time()\n",
    "rewards, actions = simulateBandits(agents, arms)\n",
    "print(\"Elapsed simulation time: {} min\".format((time() - start)/60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot achieved rewards and actions\n",
    "fig, ax = plt.subplots(3,1, sharex='all', figsize=(16, 14))\n",
    "\n",
    "ax[0].set_ylabel(\"average reward\")\n",
    "for i, a in enumerate(agents):\n",
    "    if a.decision_rule == \"epsilon-greedy\":\n",
    "        ax[0].plot(rewards[i, :], label='epsilon = {}'.format(a.epsilon))\n",
    "    elif a.decision_rule == \"UCB\":\n",
    "        ax[0].plot(rewards[i, :], label='UCB param = {}'.format(a.UCB_param))\n",
    "    else:\n",
    "        ax[0].plot(rewards[i, :], label=a.decision_rule)\n",
    "\n",
    "ax[1].set_ylabel(\"cumulative reward\")\n",
    "for i, a in enumerate(agents):\n",
    "    if a.decision_rule == \"epsilon-greedy\":\n",
    "        ax[1].plot(np.cumsum(rewards[i, :]), label='epsilon = {}'.format(a.epsilon))\n",
    "    elif a.decision_rule == \"UCB\":\n",
    "        ax[1].plot(np.cumsum(rewards[i, :]), label='UCB param = {}'.format(a.UCB_param))\n",
    "    else:\n",
    "        ax[1].plot(np.cumsum(rewards[i, :]), label=a.decision_rule)\n",
    "\n",
    "ax[2].set_ylabel(\"average best arm picks\")\n",
    "for i, a in enumerate(agents):\n",
    "    if a.decision_rule == \"epsilon-greedy\":\n",
    "        ax[2].plot(actions[i, :], label='epsilon = {}'.format(a.epsilon))\n",
    "    elif a.decision_rule == \"UCB\":\n",
    "        ax[2].plot(actions[i, :], label='UCB param = {}'.format(a.UCB_param))\n",
    "    else:\n",
    "        ax[2].plot(actions[i, :], label=a.decision_rule)\n",
    "\n",
    "\n",
    "# Shrink current axis's height by 10% on the bottom\n",
    "box = ax[2].get_position()\n",
    "ax[2].set_position([box.x0, box.y0 + box.height * 0.1,\n",
    "                 box.width, box.height * 0.9])\n",
    "\n",
    "# Put a legend below current axis\n",
    "ax[2].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05),\n",
    "          fancybox=True, shadow=True, ncol=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

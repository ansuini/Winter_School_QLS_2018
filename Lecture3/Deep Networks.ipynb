{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras as k\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "from keras.datasets import mnist\n",
    "from keras import layers\n",
    "from keras.models import Model, load_model\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import callbacks\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import regularizers\n",
    "import os.path as path\n",
    "import os\n",
    "import pickle\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "jtplot.style(grid=False, ticks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test(model, validation_data):\n",
    "    labels = np.argmax(model.predict(validation_data[0]), axis=1)\n",
    "    ground_truth = np.argmax(validation_data[1], axis=1)\n",
    "    accuracy = np.sum(labels == ground_truth)/labels.size\n",
    "    num_errors = min(np.sum(labels != ground_truth), 20)\n",
    "    errors = np.random.choice(np.nonzero(labels != ground_truth)[0], num_errors, replace=False)\n",
    "    correct = np.random.choice(\n",
    "        np.nonzero(labels == ground_truth)[0], 60 - num_errors, replace=False)\n",
    "    stimuli = np.hstack((errors, correct))\n",
    "    np.random.shuffle(stimuli)\n",
    "    plt.style.use('grayscale')\n",
    "    num_columns = 10\n",
    "    num_rows = 6\n",
    "    fig, axes = plt.subplots(num_rows, 10, figsize=(20, 3 * num_rows))\n",
    "    for idx, ax in zip(stimuli, axes.ravel()):\n",
    "        if idx in errors:\n",
    "            c = 'r'\n",
    "        else:\n",
    "            c = 'k'\n",
    "        ax.matshow(validation_data[0][idx, :, :, 0])\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title('Prediction: '+str(labels[idx])+\n",
    "                     ' \\n Truth: '+str(ground_truth[idx]), color=c)\n",
    "    plt.suptitle(\"Model accuracy: \"+str(accuracy*100)+\" %\")\n",
    "    plt.show()\n",
    "    jtplot.style(grid=False, ticks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(mnist.load_data)\n",
    "\n",
    "(i_train, l_train), (i_test, l_test) = mnist.load_data()\n",
    "i_train, i_test = i_train/255.0, i_test/255.0\n",
    "i_train, i_test = i_train.astype(np.float32), i_test.astype(np.float32)\n",
    "i_train, i_test = np.expand_dims(i_train, 3), np.expand_dims(i_test, 3)\n",
    "l_train, l_test = to_categorical(l_train), to_categorical(l_test)\n",
    "i_validate = i_train[50000:, :, :, :]\n",
    "i_train = i_train[0:50000, :, :, :]\n",
    "l_validate = l_train[50000:, :]\n",
    "l_train = l_train[0:50000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_mnist(i_train, l_train, transf = [(1, 0), (-1, 0), (0, 1), (0, -1)]):\n",
    "    i_enhanced = []\n",
    "    l_enhanced = []\n",
    "    for i in range(i_train.shape[0]):\n",
    "        img = i_train[i, :, :, :]\n",
    "        label = l_train[i, :]\n",
    "        i_enhanced.append(img)\n",
    "        l_enhanced.append(label)\n",
    "        for shift in transf:\n",
    "            x = np.pad(img, ((abs(shift[0]), abs(shift[0])),\n",
    "                         (abs(shift[1]), abs(shift[1])), (0, 0)), 'constant')\n",
    "            x = np.roll(x, shift, (0, 1))\n",
    "            x = x[abs(shift[0]):(x.shape[0]-abs(shift[0])),\n",
    "                  abs(shift[1]):(x.shape[1]-abs(shift[1])), :]\n",
    "            i_enhanced.append(x)\n",
    "            l_enhanced.append(label)\n",
    "    i_enhanced = np.array(i_enhanced)\n",
    "    l_enhanced = np.array(l_enhanced)\n",
    "    perm_idx = np.random.permutation(i_enhanced.shape[0])\n",
    "    return i_enhanced[perm_idx, :, :, :], l_enhanced[perm_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "\n",
    "if path.isfile(path.join('data', 'training_hist.pkl')):\n",
    "    with open(path.join('data', 'training_hist.pkl'), 'rb') as f:\n",
    "        history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_size = 10\n",
    "n_epochs = 60\n",
    "learn_rate = 0.1\n",
    "history = {}\n",
    "tb_params = {\n",
    "    'write_images': True, 'histogram_freq': 5, \n",
    "    'write_grads': True, 'write_graph': False\n",
    "            }\n",
    "activation_fcn = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "img = layers.Input(shape=(28, 28, 1,), name='images')\n",
    "x = layers.Flatten()(img)\n",
    "x = layers.Dense(100, activation=activation_fcn, name='hidden')(x)\n",
    "out = layers.Dense(10, activation='softmax', name='output')(x)\n",
    "\n",
    "model = Model(img, out)\n",
    "opt = optimizers.SGD(lr=learn_rate)\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "tb = callbacks.TensorBoard(log_dir=path.join('tb_data', 'fully_connected'), **tb_params)\n",
    "h = model.fit(\n",
    "    i_train, l_train, validation_data=(i_validate, l_validate), epochs=n_epochs,\n",
    "    batch_size=minibatch_size, callbacks=[tb]\n",
    "    )\n",
    "history['Fully Connected'] = h.history\n",
    "history['Fully Connected']['epoch'] = h.epoch\n",
    "model.save(path.join('data', 'fully_connected.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history['Fully Connected']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(hist['epoch'], hist['acc'])\n",
    "ax[0].plot(hist['epoch'], hist['val_acc'])\n",
    "ax[0].legend(['Training Accuracy','Validation Accuracy'])\n",
    "ax[1].plot(hist['epoch'], hist['loss'])\n",
    "ax[1].plot(hist['epoch'], hist['val_loss'])\n",
    "ax[1].legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = load_model(path.join('data', 'fully_connected.h5'))\n",
    "show_test(model, (i_test, l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "img = layers.Input(shape=(28,28,1,))\n",
    "x = layers.Conv2D(20, 5, activation=activation_fcn)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(100, activation=activation_fcn)(x)\n",
    "out = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(img, out)\n",
    "opt = optimizers.SGD(lr=learn_rate)\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "tb = callbacks.TensorBoard(log_dir=path.join('tb_data', 'convolutional'), **tb_params)\n",
    "h = model.fit(\n",
    "    i_train, l_train, validation_data=(i_validate, l_validate), epochs=n_epochs,\n",
    "    batch_size=minibatch_size, callbacks=[tb]\n",
    "    )\n",
    "history['Convolutional'] = h.history\n",
    "history['Convolutional']['epoch'] = h.epoch\n",
    "model.save(path.join('data', 'convolutional.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history['Convolutional']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(hist['epoch'], hist['acc'])\n",
    "ax[0].plot(hist['epoch'], hist['val_acc'])\n",
    "ax[0].legend(['Training Accuracy','Validation Accuracy'])\n",
    "ax[1].plot(hist['epoch'], hist['loss'])\n",
    "ax[1].plot(hist['epoch'], hist['val_loss'])\n",
    "ax[1].legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = load_model(path.join('data', 'convolutional.h5'))\n",
    "show_test(model, (i_test, l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "img = layers.Input(shape=(28,28,1,))\n",
    "x = layers.Conv2D(20, 5, activation=activation_fcn)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(40, 5, activation=activation_fcn)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(100, activation=activation_fcn)(x)\n",
    "out = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(img, out)\n",
    "opt = optimizers.SGD(lr=learn_rate)\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "tb = callbacks.TensorBoard(log_dir=path.join('tb_data', 'convolutional_2layers'), **tb_params)\n",
    "h = model.fit(\n",
    "    i_train, l_train, validation_data=(i_validate, l_validate), epochs=n_epochs,\n",
    "    batch_size=minibatch_size, callbacks=[tb]\n",
    "    )\n",
    "history['Convolutional (2 Layers)'] = h.history\n",
    "history['Convolutional (2 Layers)']['epoch'] = h.epoch\n",
    "model.save(path.join('data', 'convolutional_2layers.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history['Convolutional (2 Layers)']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(hist['epoch'], hist['acc'])\n",
    "ax[0].plot(hist['epoch'], hist['val_acc'])\n",
    "ax[0].legend(['Training Accuracy','Validation Accuracy'])\n",
    "ax[1].plot(hist['epoch'], hist['loss'])\n",
    "ax[1].plot(hist['epoch'], hist['val_loss'])\n",
    "ax[1].legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = load_model(path.join('data', 'convolutional_2layers.h5'))\n",
    "show_test(model, (i_test, l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_enh, l_enh = enhance_mnist(i_train, l_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fcn = 'relu'\n",
    "lmbd = 0.01\n",
    "learn_rate = 0.03\n",
    "minibatch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "regularization = regularizers.l2(lmbd)\n",
    "\n",
    "img = layers.Input(shape=(28,28,1,))\n",
    "x = layers.Conv2D(20, 5, activation=activation_fcn, kernel_regularizer=regularization)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(40, 5, activation=activation_fcn, kernel_regularizer=regularization)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(100, activation=activation_fcn, kernel_regularizer=regularization)(x)\n",
    "out = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(img, out)\n",
    "opt = optimizers.SGD(lr=learn_rate)\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "tb = callbacks.TensorBoard(log_dir=path.join('tb_data', 'convolutional_relu'), **tb_params)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001, verbose=True)\n",
    "h = model.fit(\n",
    "    i_enh, l_enh, validation_data=(i_validate, l_validate), epochs=n_epochs,\n",
    "    batch_size=minibatch_size, callbacks=[tb, reduce_lr]\n",
    "    )\n",
    "history['Convolutional (ReLU)'] = h.history\n",
    "history['Convolutional (ReLU)']['epoch'] = h.epoch\n",
    "model.save(path.join('data', 'convolutional_relu.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history['Convolutional (ReLU)']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(hist['epoch'], hist['acc'])\n",
    "ax[0].plot(hist['epoch'], hist['val_acc'])\n",
    "ax[0].legend(['Training Accuracy','Validation Accuracy'])\n",
    "ax[1].plot(hist['epoch'], hist['loss'])\n",
    "ax[1].plot(hist['epoch'], hist['val_loss'])\n",
    "ax[1].legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = load_model(path.join('data', 'convolutional_relu.h5'))\n",
    "show_test(model, (i_test, l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "regularization = regularizers.l2(lmbd)\n",
    "\n",
    "img = layers.Input(shape=(28,28,1,))\n",
    "x = layers.Conv2D(20, 5, activation=activation_fcn, kernel_regularizer=regularization)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(40, 5, activation=activation_fcn, kernel_regularizer=regularization)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(100, activation=activation_fcn, kernel_regularizer=regularization)(x)\n",
    "x = layers.Dense(100, activation=activation_fcn, kernel_regularizer=regularization)(x)\n",
    "out = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(img, out)\n",
    "opt = optimizers.SGD(lr=learn_rate)\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "tb = callbacks.TensorBoard(log_dir=path.join('tb_data', 'convolutional_2dense'), **tb_params)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001, verbose=True)\n",
    "\n",
    "h = model.fit(\n",
    "    i_enh, l_enh, validation_data=(i_validate, l_validate), epochs=n_epochs,\n",
    "    batch_size=minibatch_size, callbacks=[tb, reduce_lr]\n",
    "    )\n",
    "history['Convolutional (2 Dense Layers)'] = h.history\n",
    "history['Convolutional (2 Dense Layers)']['epoch'] = h.epoch\n",
    "model.save(path.join('data', 'convolutional_2dense.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history['Convolutional (2 Dense Layers)']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(hist['epoch'], hist['acc'])\n",
    "ax[0].plot(hist['epoch'], hist['val_acc'])\n",
    "ax[0].legend(['Training Accuracy','Validation Accuracy'])\n",
    "ax[1].plot(hist['epoch'], hist['loss'])\n",
    "ax[1].plot(hist['epoch'], hist['val_loss'])\n",
    "ax[1].legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = load_model(path.join('data', 'convolutional_2dense.h5'))\n",
    "show_test(model, (i_test, l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "regularization = None#regularizers.l2(lmbd)\n",
    "\n",
    "img = layers.Input(shape=(28,28,1,))\n",
    "x = layers.Conv2D(20, 5, activation=activation_fcn, kernel_regularizer=regularization)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Conv2D(40, 5, activation=activation_fcn, kernel_regularizer=regularization)(img)\n",
    "x = layers.MaxPool2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(1000, activation=activation_fcn, kernel_regularizer=regularization)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(1000, activation=activation_fcn, kernel_regularizer=regularization)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "out = layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "model = Model(img, out)\n",
    "opt = optimizers.SGD(lr=learn_rate)\n",
    "model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "tb = callbacks.TensorBoard(log_dir=path.join('tb_data', 'convolutional_dropout'), **tb_params)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=5, min_lr=0.001, verbose=True)\n",
    "\n",
    "h = model.fit(\n",
    "    i_enh, l_enh, validation_data=(i_validate, l_validate), epochs=n_epochs,\n",
    "    batch_size=minibatch_size, callbacks=[tb, reduce_lr]\n",
    "    )\n",
    "history['Convolutional (Dropout)'] = h.history\n",
    "history['Convolutional (Dropout)']['epoch'] = h.epoch\n",
    "model.save(path.join('data', 'convolutional_dropout.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history['Convolutional (Dropout)']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "ax[0].plot(hist['epoch'], hist['acc'])\n",
    "ax[0].plot(hist['epoch'], hist['val_acc'])\n",
    "ax[0].legend(['Training Accuracy','Validation Accuracy'])\n",
    "ax[1].plot(hist['epoch'], hist['loss'])\n",
    "ax[1].plot(hist['epoch'], hist['val_loss'])\n",
    "ax[1].legend(['Training Loss','Validation Loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = load_model(path.join('data', 'convolutional_dropout.h5'))\n",
    "show_test(model, (i_test, l_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path.join('data', 'training_hist.pkl'), 'wb') as f:\n",
    "    pickle.dump(history, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Plots\n",
    "============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, hist in history.items():\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].plot(hist['epoch'], hist['acc'])\n",
    "    ax[0].plot(hist['epoch'], hist['val_acc'])\n",
    "    ax[0].legend(['Training Accuracy','Validation Accuracy'])\n",
    "    ax[1].plot(hist['epoch'], hist['loss'])\n",
    "    ax[1].plot(hist['epoch'], hist['val_loss'])\n",
    "    ax[1].legend(['Training Loss','Validation Loss'])\n",
    "    fig.suptitle(name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.6.0",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

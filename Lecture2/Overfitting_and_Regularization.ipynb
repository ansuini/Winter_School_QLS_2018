{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements and Regularization techniques\n",
    "\n",
    "Before we start a fun thing : this is the usual way you see MNIST\n",
    "\n",
    "<img src=\"figs/mnist_100_digits.png\" alt=\"drawing\" width=\"500\" >\n",
    "\n",
    "\n",
    "These are some MNIST data visualized with a technique called T-SNE\n",
    "\n",
    "<img src=\"figs/mnist_tsne.png\" alt=\"drawing\" width=\"500\" >\n",
    "\n",
    "\n",
    "This technique is helpful to make sense of high-dimensional data.\n",
    "Are you curious about this ? \n",
    "\n",
    "Find out more here http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this notebook about\n",
    "\n",
    "\n",
    "************************************************************************************\n",
    "\n",
    "First of all : a disclaimer.\n",
    "\n",
    "The way we thought about these notebooks is that during the School\n",
    "you will try it and *break the ice* with deep learning.\n",
    "In a second moment, after the School is finished, or during the School\n",
    "if you have time, you will be able to explore them more by yourself.\n",
    "The first part is largely based on Nielsen's book and code (that we warmly\n",
    "suggest to study extensively). This is valid until we reach \n",
    "the lecture on convolutional networks, then we will depart from Nielsen's book.\n",
    "\n",
    "You will be able to run all the codes that we will provide from within the virtual\n",
    "machine.\n",
    "If you want to create an environment by yourself - without the VM - you\n",
    "can try installing the packages manually following the list of requirements\n",
    "that we provided.\n",
    "Sometimes this will be tricky and maybe you will have to play a bit\n",
    "with versions etc. but ... that's life !\n",
    "\n",
    "End of the disclaimer.\n",
    "\n",
    "************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plan\n",
    "\n",
    "\n",
    "Here is the plan for today : \n",
    "\n",
    "- We will first of all improve the backpropagation algorithm\n",
    "introducing a new cost function : the cross-entropy.\n",
    "\n",
    "\n",
    "- We will look at the code in order to see where it is defined and how.\n",
    "This will be useful if you want to define a new loss function for your\n",
    "own purposes and to make yourself comfortable with the code.\n",
    "Modifying existing codes, seeing them working and improving it \n",
    "is and highly rewarding experience.\n",
    "\n",
    "- We will discuss cross-validation and how to choose for hyperparameters in\n",
    "a simple case\n",
    "\n",
    "- We will then play with regularization (L2) in real networks\n",
    " \n",
    "- We will discuss the difference between L2 and L1 in terms of how do\n",
    "they influence the weights evolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The original code\n",
    "\n",
    "\n",
    "Here is the original code. \n",
    "Let us first take a look at it an read a bit the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"network2.py\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "An improved version of network.py, implementing the stochastic\n",
    "gradient descent learning algorithm for a feedforward neural network.\n",
    "Improvements include the addition of the cross-entropy cost function,\n",
    "regularization, and better initialization of network weights.  Note\n",
    "that I have focused on making the code simple, easily readable, and\n",
    "easily modifiable.  It is not optimized, and omits many desirable\n",
    "features.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereby the loss functions are defined. Notice that they are defined as classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define the quadratic and cross-entropy cost functions\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n",
    "        in the same slot, then the expression (1-y)*np.log(1-a)\n",
    "        returns nan.  The np.nan_to_num ensures that that is converted\n",
    "        to the correct value (0.0).\n",
    "\n",
    "        \"\"\"\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.  Note that the\n",
    "        parameter ``z`` is not used by the method.  It is included in\n",
    "        the method's parameters in order to make the interface\n",
    "        consistent with the delta method for other cost classes.\n",
    "\n",
    "        \"\"\"\n",
    "        return (a-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens if we take the logarithm of zero ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decomment here to read the docs\n",
    "\n",
    "#?np.nan_to_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log(zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.nan_to_num(np.log(zero)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the respective\n",
    "        layers of the network.  For example, if the list was [2, 3, 1]\n",
    "        then it would be a three-layer network, with the first layer\n",
    "        containing 2 neurons, the second layer 3 neurons, and the\n",
    "        third layer 1 neuron.  The biases and weights for the network\n",
    "        are initialized randomly, using\n",
    "        ``self.default_weight_initializer`` (see docstring for that\n",
    "        method).\n",
    "\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "        \"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1 over the square root of the number of\n",
    "        weights connecting to the same neuron.  Initialize the biases\n",
    "        using a Gaussian distribution with mean 0 and standard\n",
    "        deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
    "        and standard deviation 1.  Initialize the biases using a\n",
    "        Gaussian distribution with mean 0 and standard deviation 1.\n",
    "\n",
    "        Note that the first layer is assumed to be an input layer, and\n",
    "        by convention we won't set any biases for those neurons, since\n",
    "        biases are only ever used in computing the outputs from later\n",
    "        layers.\n",
    "\n",
    "        This weight and bias initializer uses the same approach as in\n",
    "        Chapter 1, and is included for purposes of comparison.  It\n",
    "        will usually be better to use the default weight initializer\n",
    "        instead.\n",
    "\n",
    "        \"\"\"\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0,\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "        training_data = list(training_data)\n",
    "        if evaluation_data: \n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)            \n",
    "        n = len(training_data)\n",
    "        \n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print(\"Epoch %s training complete\" % j )\n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)\n",
    "                print(\"Cost on training data: {}\".format(cost) )\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n) )\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print(\"Cost on evaluation data: {}\".format(cost) )\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print(\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data) )\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "        \"\"\"Update the network's weights and biases by applying gradient\n",
    "        descent using backpropagation to a single mini batch.  The\n",
    "        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n",
    "        learning rate, ``lmbda`` is the regularization parameter, and\n",
    "        ``n`` is the total size of the training data set.\n",
    "\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "        \"\"\"Return the total cost for the data set ``data``.  The flag\n",
    "        ``convert`` should be set to False if the data set is the\n",
    "        training data (the usual case), and to True if the data set is\n",
    "        the validation or test data.  See comments on the similar (but\n",
    "        reversed) convention for the ``accuracy`` method, above.\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"Save the neural network to the file ``filename``.\"\"\"\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Loading a Network\n",
    "def load(filename):\n",
    "    \"\"\"Load a neural network from the file ``filename``.  Returns an\n",
    "    instance of Network.\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "    and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "    into a corresponding desired output from the neural network.\n",
    "\n",
    "    \"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the cross-entropy to classify MNIST digits\n",
    "\n",
    "\n",
    "Let us first show that cross-entropy works. This means that we just\n",
    "want to verify that the network is learning something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader \n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate network of this form\n",
    "\n",
    "- 30 hidden neurons\n",
    "- mini-batch size of 10\n",
    "- $\\eta=0.5$ \n",
    "\n",
    "<img src=\"figs/tikz12.png\" alt=\"drawing\" width=\"400\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network2\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we evaluate the code in the cells above we already\n",
    "have the Network and CrossEntropyCost classes available.\n",
    "\n",
    "It would have been sufficient to write then\n",
    "\n",
    "``net = Network([784, 30, 10], cost=CrossEntropyCost)``\n",
    " \n",
    " Let us do this way anyway !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.large_weight_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 10\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, evaluation_accuracy, _, training_accuracy = net.SGD(training_data, epochs, batch_size , learning_rate, \n",
    "                                                 evaluation_data=test_data,\n",
    "                                                 monitor_evaluation_accuracy=True,\n",
    "                                                 monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(0, epochs, 1), np.array(training_accuracy)/50000, '-o',color='#FFA933',\n",
    "            label=\"Accuracy on the training data\")\n",
    "ax.plot(np.arange(0, epochs, 1), np.array(evaluation_accuracy)/10000, '-o', color='#2A6EA6', \n",
    "            label=\"Accuracy on the test data\")\n",
    "ax.set_xlim(0, epochs)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylim(0.9, 1)\n",
    "ax.set_title('Classification accuracy')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the cross-entropy works well, as we already showed for the toy\n",
    "model with just 1 neuron.\n",
    "\n",
    "\n",
    "Let us save this model in a dictionary with sizes, weights etc...\n",
    "\n",
    "******************************************************************************\n",
    "Important !\n",
    "\n",
    "Be careful that this will overwrite your results, so if you want\n",
    "to preserve the models already stored you will have to change\n",
    "names or make a check of existence of the files you are\n",
    "trying to save.\n",
    "******************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('models/net1_5_epochs')\n",
    "\n",
    "np.save('results/training_accuracy1_5_epochs.npy', training_accuracy)\n",
    "np.save('results/evaluation_accuracy1_5_epochs.npy', evaluation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can then load the network from the saved list of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net\n",
    "\n",
    "net = load('models/net1_5_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "\n",
    "We consider now a situation where our network overfits.\n",
    "The training will be done on a small subset of the data.\n",
    "\n",
    "To reproduce these data you could launch the script overfitting.py\n",
    "\n",
    "- same network\n",
    "- training on 1000 images \n",
    "- 400 epochs\n",
    "\n",
    "We will not do that here, you can do it afterwards.\n",
    "What you will obtain is ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/overfitting1.png\" alt=\"drawing\" width=\"600\" >\n",
    "\n",
    "\n",
    "<img src=\"figs/overfitting2.png\" alt=\"drawing\" width=\"600\" >\n",
    "\n",
    "we realize that around epoch $\\sim 280$ the network overfits : \"what our network learns after epoch 280 no longer generalizes to the test data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the test accuracy :\n",
    "\n",
    "<img src=\"figs/overfitting4.png\" alt=\"drawing\" width=\"600\" >\n",
    "\n",
    "we can see that the network is memorizing the training set ($100 \\%$ accuracy), without understanding the structure of the data ($\\sim 82 \\%$ accuracy on the test set)\n",
    "\n",
    "\n",
    "If we trained the network ``net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)`` with all the data for 30 epoch (and not 5 as we did) \n",
    "we would have obtained a plot similar to this\n",
    "\n",
    "<img src=\"figs/early_stopping1.png\" alt=\"drawing\" width=\"400\" >\n",
    "\n",
    "\n",
    "Overfitting is much less than in the case in which we train on 1000 images, but it's still there.\n",
    "\n",
    "This suggests that a strategy to reduce overfitting is to increase the size of the training data, also if this required to create new data in somewhat artificial ways : this is called data *augmentation*, and a simple form of it will be described in the lecture on\n",
    "convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to cure overfitting : regularization techniques\n",
    "\n",
    "- Early stopping\n",
    "- Weight decay\n",
    "- Dropout \n",
    "- Data augmentation\n",
    "\n",
    "\n",
    "In the following we will sketch how to implement the early stopping and then focus on a particular regularization technique : L2 weight decay regularization.\n",
    "\n",
    "The dropout and a very simple form of data augmentation technique will be introduced practically in the lecture on convolutional networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "\n",
    "The simplest implementation of early stopping does not require much work.\n",
    "\n",
    "You have to :\n",
    "\n",
    "- keep track of accuracy on training and validation data\n",
    "- make a plot of these two quantities as a function of the epoch\n",
    "- decide a stopping criterion\n",
    "\n",
    "To decide a stopping criterion can be easy in idealized situations\n",
    "like in the following picture\n",
    "\n",
    "\n",
    "<img src=\"figs/early_stopping.png\" alt=\"drawing\" width=\"400\" >\n",
    "\n",
    "\n",
    "but in reality there are fluctuations, plateaus and all kind of wild \n",
    "behaviors (check if you are interested the Andrei Karpathy freaky\n",
    "collection here : https://lossfunctions.tumblr.com/)\n",
    "\n",
    "This is a quite *normal* overfitting example evidenced with the \n",
    "behavior of training and test (validation) loss\n",
    "\n",
    "<img src=\"figs/training_cifar10.png\" alt=\"drawing\" width=\"400\" >\n",
    "\n",
    "on a small network training on the cifar10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "\n",
    "\n",
    "You can easily realize that a stopping criterion has to be defined.\n",
    "It could be :\n",
    "\n",
    "- stop when the minimum (determined in the aftermath) has been reached \n",
    "- stop when the loss/error in the validation set does not decrease anymore\n",
    "in an observation window of your choice \n",
    "- ...\n",
    "\n",
    "and an infinity of variations.\n",
    "\n",
    "\n",
    "We will sketch the code to implement the first version of early stopping now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guided exercise : early stopping\n",
    "\n",
    "We will sketch the code to implement early stopping by looking at the accuracies\n",
    "in the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper() \n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "test_data = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "net.large_weight_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ev_acc, _, tr_acc  = net.SGD(training_data[:1000], 1, 10, 0.5, \n",
    "                                                       evaluation_data=test_data, lmbda = 0.1,\n",
    "                                                       monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,\n",
    "                                                       monitor_training_cost=True, monitor_training_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "Ev_acc = []\n",
    "Tr_acc = []\n",
    "\n",
    "best_ev_acc = 0.0\n",
    "best_epoch = 0\n",
    "best_model = net\n",
    "\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "    _, ev_acc, _, tr_acc  = net.SGD(training_data[:1000], 1, 10, 0.5, \n",
    "                                                       evaluation_data=test_data, lmbda = 0.1,\n",
    "                                                       monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,\n",
    "                                                       monitor_training_cost=True, monitor_training_accuracy=True)\n",
    "\n",
    "    Ev_acc.append(ev_acc[0])\n",
    "    Tr_acc.append(tr_acc[0])\n",
    "    \n",
    "    # if the accuracy on the validation data is better than in the \n",
    "    # previous model I declare the the current model is the best \n",
    "    # and I also keep trach of the epoch\n",
    "    \n",
    "    if ev_acc[0] > best_ev_acc:\n",
    "        \n",
    "        best_ev_acc = ev_acc[0]\n",
    "        best_epoch = i\n",
    "        best_model = net\n",
    "        \n",
    "# if you want to save your best model at the end of the best epoch search      \n",
    "# if SAVE:\n",
    "    #net.save('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(0, epochs, 1), np.array(Tr_acc)/1000, '-o',color='#FFA933',\n",
    "            label=\"Accuracy on the training data\")\n",
    "ax.plot(np.arange(0, epochs, 1), np.array(Ev_acc)/10000, '-o', color='#2A6EA6', \n",
    "            label=\"Accuracy on the test data\")\n",
    "\n",
    "ax.plot(best_epoch, best_ev_acc/10000, 'ro', markersize=10)\n",
    "\n",
    "ax.set_xlim(0, epochs)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Early stop at : {}'.format(best_epoch))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Homework Exercise\n",
    " \n",
    " You can try to modify your code \n",
    " \n",
    " - augmenting the data\n",
    " - modifying the plotting code accordingly (Why is this necessary ?)\n",
    " - writing the part that save the net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More generally on overfitting and hyper-parameters search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we used validation data to determine the best epoch to stop.\n",
    "Then what we would do is to evaluate the network on the test data and this \n",
    "performance (accuracy) will be the number that genuinely describe\n",
    "the generalization performance of our network.\n",
    "\n",
    "(Do it by yourself when you have time !)\n",
    "\n",
    "\n",
    "Validation data is used in general to determine the hyper-parameters such as:\n",
    "\n",
    "- number of epochs of training (as we did now)\n",
    "- learning rate\n",
    "- architecture\n",
    "\n",
    "and also (see below)\n",
    "\n",
    "- weight decay constant\n",
    "- dropout level \n",
    "\n",
    "This is important in order to avoid overfitting the hyper-parameters\n",
    "themselves to the test data.\n",
    "\n",
    "Let us now depart from the main lecture for a while to go deeper into this \n",
    "aspect.\n",
    "\n",
    "Let us open the `plot_underfitting_overfitting` notebook first\n",
    "and then `plot_underfitting_overfitting_modified`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have a bit clearer picture of what is in general overfitting and how \n",
    "to use validation data in order to tame model complexity at least in the \n",
    "case of polynomial approximation.\n",
    "\n",
    "We also saw how in the particular case of networks how\n",
    "to determine the best epoch to stop the training before the overfitting onset.\n",
    "\n",
    "Now we will take a look at the weight decay regularization in the L2 flavour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise :\n",
    "\n",
    "- Try to localize the parts of the code in which the L2 regularization is implemented\n",
    "  and tell what is the line where these parts appear and their role\n",
    "  (learning rule, computation of the cost, initializations...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "epochs = 400\n",
    "\n",
    "if TRAIN:\n",
    "    training_data, validation_data, test_data = mnist_loader.load_data_wrapper() \n",
    "    training_data = list(training_data)\n",
    "    validation_data = list(validation_data)\n",
    "    test_data = list(test_data)\n",
    "    net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "    net.large_weight_initializer()\n",
    "    _, evaluation_accuracy, _, training_accuracy = net.SGD(training_data[:1000], epochs, 10, 0.5, \n",
    "                                                           evaluation_data=test_data, lmbda = 0.1,\n",
    "                                                           monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,\n",
    "                                                           monitor_training_cost=True, monitor_training_accuracy=True)\n",
    "\n",
    "    net.save('models/net2')\n",
    "    np.save('results/training_accuracy2.npy', training_accuracy)\n",
    "    np.save('results/evaluation_accuracy2.npy', evaluation_accuracy)\n",
    "else:\n",
    "    net = network2.load('models/net2')\n",
    "    training_accuracy = np.load('results/training_accuracy2.npy')\n",
    "    evaluation_accuracy = np.load('results/evaluation_accuracy2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(200, epochs, 1), np.array(evaluation_accuracy)[200:]/10000, color='#2A6EA6', \n",
    "            label=\"Accuracy on the test data\")\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Accuracy (%) on test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the use of $L_2$ regularization has suppressed overfitting.\n",
    "Accuracy gets better $\\sim 82 \\% \\rightarrow \\sim 87 \\%$\n",
    "\n",
    "We can now go back to the initial situation with the complete training dataset and\n",
    "see how much we can improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise \n",
    "\n",
    "\n",
    "- When you apply the weight decay to the whole dataset you have to\n",
    "change the weight decay coefficient lambda : can you see why ?\n",
    "Hint : take a look at the modified learning rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "epochs = 30\n",
    "batch_size = 10\n",
    "learning_rate = 0.5\n",
    "lmbda = 5.0\n",
    "\n",
    "if TRAIN:\n",
    "    training_data, validation_data, test_data = mnist_loader.load_data_wrapper() \n",
    "    training_data = list(training_data)\n",
    "    validation_data = list(validation_data)\n",
    "    test_data = list(test_data)\n",
    "    net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
    "    net.large_weight_initializer()\n",
    "    \n",
    "    \n",
    "    _, evaluation_accuracy, _, training_accuracy = net.SGD(training_data, epochs, batch_size , learning_rate, \n",
    "                                                 evaluation_data=test_data,\n",
    "                                                 lmbda = lmbda,\n",
    "                                                 #monitor_evaluation_cost=True, \n",
    "                                                 monitor_evaluation_accuracy=True,\n",
    "                                                 #monitor_training_cost=True, \n",
    "                                                 monitor_training_accuracy=True)\n",
    "    \n",
    "\n",
    "    net.save('models/net3')\n",
    "    np.save('results/training_accuracy3.npy', training_accuracy)\n",
    "    np.save('results/evaluation_accuracy3.npy', evaluation_accuracy)\n",
    "else:\n",
    "    net = network2.load('models/net3')\n",
    "    training_accuracy = np.load('results/training_accuracy3.npy')\n",
    "    evaluation_accuracy = np.load('results/evaluation_accuracy3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(0, epochs, 1), np.array(training_accuracy)/50000, color='#FFA933',\n",
    "            label=\"Accuracy on the training data\")\n",
    "ax.plot(np.arange(0, epochs, 1), np.array(evaluation_accuracy)/10000, color='#2A6EA6', \n",
    "            label=\"Accuracy on the test data\")\n",
    "ax.set_xlim(0, epochs)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_title('Classification accuracy')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on the test data improved and the gap between the accuracy on test and train data\n",
    "is narrower (it is now $\\sim 1 \\%$, it was $\\sim 2 \\%$ without regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises \n",
    "\n",
    "\n",
    "- 1. (Reading) : read the discussion of further benefits of regularization at the end of the corresponding paragraph in Nielsen's book (it is worth it !)\n",
    "\n",
    "- 2. (To be done in class) Consider the $L_1$ regularization. Derive the learning rule and provide an heuristic argument \n",
    "to show that $L_1$ regularization \"tends to concentrate the weights of the network in a relatively small number of high-importance connections, while the other weights are driven toward zero.\" What can be a possible computational verification of this statement ?\n",
    "\n",
    "- 3. (If time allows) Consider the $L2$ regularization and sketch an hyper-parameters search over\n",
    "the $\\lambda$ space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sketch of exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper() \n",
    "training_data = list(training_data)\n",
    "validation_data = list(validation_data)\n",
    "test_data = list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# istantiate network class\n",
    "net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a number of epochs and a set of hyper-parameters to explore\n",
    "epochs = 20\n",
    "lmbdas = [0.005, 0.05, 0.5, 5., 50.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ev_acc = []\n",
    "Tr_acc = []\n",
    "\n",
    "for lmbda in lmbdas:\n",
    "    \n",
    "    net.large_weight_initializer()\n",
    "    \n",
    "    ev_acc = []\n",
    "    tr_acc = []\n",
    "    for i in range(epochs):        \n",
    "        _, evaluation_accuracy, _, training_accuracy = net.SGD(training_data, 1, batch_size , learning_rate, \n",
    "                                                     evaluation_data=test_data,\n",
    "                                                     lmbda = lmbda,\n",
    "                                                     #monitor_evaluation_cost=True, \n",
    "                                                     monitor_evaluation_accuracy=True,\n",
    "                                                     #monitor_training_cost=True, \n",
    "                                                     monitor_training_accuracy=True)\n",
    "        ev_acc.append(evaluation_accuracy[0])\n",
    "        tr_acc.append(training_accuracy[0])\n",
    "    Ev_acc.append(ev_acc)\n",
    "    Tr_acc.append(tr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ev_acc = np.array(Ev_acc)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "for i in range(len(lmbdas)):\n",
    "    plt.plot(Ev_acc[i,:]/10000,'-o',label=lmbdas[i])\n",
    "plt.xticks(range(epochs))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy %')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment number of epochs and\n",
    "# iterate on the choice of lambda until you find\n",
    "# a suitable range that contain an optimal lambda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/bin/python3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-written digit recognition (MNIST database)\n",
    "\n",
    "### Based on the book by Michael Nielsen, @ http://neuralnetworksanddeeplearning.com/index.html \n",
    "\n",
    "### Code on GitHub @ https://github.com/mnielsen/neural-networks-and-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mnist_100_digits.png\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Introduction to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST dataset (how it is loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>training_data</code>, <code>validation_data</code>, <code>test_data</code> are iterators containing pairs of input-output arrays:\n",
    "* <code>training_data</code> contains 50000 pairs, and is used to train the network\n",
    "* <code>test_data</code> contains 10000 pairs, and is used to test the performance of the network;\n",
    "* <code>validation_data</code> also contains 10000 pairs and is used as extra ''testing'' data (to set hyper-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the network and calculate the optimum parameters via Stochastic Gradient Descent of the mean-square error cost function\n",
    "\n",
    "* Define a network with given numbers of nodes per layer (784 is the number of pixels of input images, 10 is the output vector)\n",
    "<br/><br/>\n",
    "\n",
    "* Define the parameters of the learning:\n",
    "    * <code>epochs</code> is the number of complete *sweeps* over the training data\n",
    "    <br/><br/>\n",
    "    \n",
    "    * <code>mini_batch_size</code> is the number of data points used for the evaluation of the gradient (i.e. between two steepest descent steps), $M$\n",
    "    <br/><br/>\n",
    "    \n",
    "    * <code>eta</code> is the learning rate, $\\eta$\n",
    "    <br/><br/>\n",
    "    \n",
    "    * $\\ldots$\n",
    "\n",
    "* Define a cost function (defined inside the <code>Network</code> class), i.e. the mean-square error function\n",
    "    \n",
    "    $$\n",
    "        C = \\frac{1}{N}\\sum_x \\underbrace{\\big\\| a^L(x) - y(x) \\big\\|^2}_{C_x}\n",
    "    $$\n",
    "    \n",
    "* **Train the network**.  For every epoch, \n",
    "\n",
    "    1. shuffle the training data (or the portion of training data passed to the SGD function) and divide them into mini-batches of size $M$;\n",
    "    <br/><br/>\n",
    "    \n",
    "    2. for each mini-batch calculate the gradient of the cost with respect to the parameters, as $ \\nabla_{w,b} C \\simeq \\overline{\\nabla_{w,b} C} $ (where $\\,\\overline{\\,\\bullet\\,}\\,$ indicates the sample mean over the mini-batch) using the **backpropagation** algorithm (see below) and take a gradient descent step as $$\n",
    "            w \\leftarrow w - \\eta\\,\\overline{\\frac{\\partial C}{\\partial w}}\n",
    "            \\qquad \\textrm{and} \\qquad\n",
    "            b \\leftarrow b - \\eta\\,\\overline{\\frac{\\partial C}{\\partial b}}$$\n",
    "    <br/><br/>\n",
    "\n",
    "    4. Test the performance with the <code>test_data</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How backpropagation works\n",
    "\n",
    "For a given input $x$:\n",
    "\n",
    "1. **Set the input** as $a^1_j(x) = x_j$\n",
    "<br/><br/>\n",
    "\n",
    "2. **Feedforward**: For all the subsequent layers, compute $a_j^L(x)$, by iterating $ a_i^l = \\sigma(z_i^l)$ where\n",
    "\n",
    "    $$ z_i^l = \\sum_j w^{l-1}_{ij} a_j^{l-1} $$\n",
    "    for all $l = 2,\\ldots L$\n",
    "<br/><br/>\n",
    "    \n",
    "3. Calculate the output **error**:\n",
    "\n",
    "    $$ \\delta_j^L(x) = \\sigma'\\big(z^L_j\\big)\\frac{\\partial C_x}{\\partial a^L_j} $$\n",
    "    <br/>\n",
    "\n",
    "4. **Propagate** the error **backward**: for all the previous layers, $l = L-1, \\ldots 2$, iterate\n",
    "\n",
    "    $$ \\delta^l_i = \\sigma'\\big(z^l_i\\big)\\,\\sum_j w_{ji}^{l+1} \\delta_j^{l+1} $$\n",
    "    <br/>\n",
    "    \n",
    "5. **Derivatives of the cost** with respect to weights and biases are\n",
    "\n",
    "    $$\n",
    "        \\frac{\\partial C_x}{\\partial w^{l}_{ij}} = a_j^{l-1} \\delta^l_i\n",
    "        \\qquad \\textrm{ and } \\qquad\n",
    "        \\frac{\\partial C_x}{\\partial b_i} = \\delta^l_i\n",
    "    $$\n",
    "    \n",
    "    \n",
    "These derivatives are accumulated into the sample mean over the mini-batch data,\n",
    "\n",
    "$$\n",
    "    \\overline{\\frac{\\partial C}{\\partial w^{l}_{ij}}} \\simeq \\frac{1}{M}\\sum_x \\frac{\\partial C_x}{\\partial w^{l}_{ij}}\n",
    "        \\qquad \\textrm{ and } \\qquad\n",
    "        \\overline{\\frac{\\partial C}{\\partial b^{l}_{j}}} \\simeq \\frac{1}{M}\\sum_x \\frac{\\partial C_x}{\\partial b^{l}_{i}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) A look at the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `mnist_loader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    \n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    - The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    - The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    \n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"\n",
    "    In order to train the neural network it is convenient to arrange\n",
    "    the training data differently.    \n",
    "    \n",
    "    Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. \n",
    "    \n",
    "    ``training_data`` is a list containing 50,000 2-tuples ``(x, y)``.\n",
    "    ``x`` is a 784-dimensional numpy.ndarray containing the input image.\n",
    "    ``y`` is a 10-dimensional numpy.ndarray representing the unit vector\n",
    "    corresponding to the correct digit for ``x``.\n",
    "       \n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the corresponding\n",
    "    classification, i.e., the digit values (integers) corresponding to ``x``.\n",
    "    \n",
    "    NOTE: the 3-tuple returned contains 'zip' objects, which are iterators.\n",
    "    They are meant to be used only once in, e.g. 'for' cycles. Once used, the\n",
    "    function 'load_data_wrapper' must be called again if we need the databases\n",
    "    for another training. Otherwise, cast the zip object into a list, e.g.\n",
    "    'training_data = list(training_data)'\n",
    "    \"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    \n",
    "    # see 'vectorized_result' function below\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    \n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `network.py`  contains the definition of the `Network` class (`Network` object and functions operating on it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        This states how the object ``Network`` is defined.\n",
    "        \n",
    "        The initializer takes as argument a list (``sizes``) containing\n",
    "        the number of neurons in the respective layers of the network.\n",
    "        \n",
    "        For example, if the list was [2, 3, 1] then it would be a\n",
    "        three-layer network, with the first layer containing 2 neurons,\n",
    "        the second layer 3 neurons, and the third layer 1 neuron.\n",
    "        \n",
    "        The biases and weights for the network are initialized randomly,\n",
    "        using a Gaussian distribution with mean 0, and variance 1.\n",
    "        \n",
    "        NOTE: the first layer is assumed to be an input layer,\n",
    "        and by convention we won't set any biases for those neurons,\n",
    "        since biases are only ever used in computing the outputs from\n",
    "        later layers.\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory. If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\n",
    "        \"\"\"\n",
    "\n",
    "        # data to be used to train the network\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        # data to check performance at the end of each epoch\n",
    "        test_data = list(test_data)\n",
    "        n_test = len(test_data)\n",
    "        \n",
    "        \n",
    "        evaluation_accuracy = []\n",
    "        training_accuracy = []\n",
    "        for j in range(epochs):\n",
    "            # shuffle to avoid effects of undesired correlations\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            # split data into ``mini_batches``\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            # each of them is used to estimate the gradient of the \n",
    "            # cost function in order to perform stochastic gradient\n",
    "            # descent (SGD)\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            correct = self.accuracy(test_data)\n",
    "            print(\"Epoch {} : {} / {}\".format(j,correct,n_test));\n",
    "            evaluation_accuracy.append(correct/n_test)\n",
    "            \n",
    "            correct = self.accuracy(training_data, convert=True)\n",
    "            training_accuracy.append(correct/n)\n",
    "            \n",
    "        return training_accuracy, evaluation_accuracy\n",
    "    \n",
    "    \n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the gradient vectors (with respect to the biases and\n",
    "        # the weights)\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        # use the minibatch (passed as argument) to evaluate the \n",
    "        # gradients through backpropagation (see function `backprop`\n",
    "        # below)\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # change the weights of the `Network` object\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        This function is the core of the learning algorithm: backpropagation\n",
    "        \n",
    "        Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.\n",
    "        ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of\n",
    "        numpy arrays, similar to ``self.biases`` and ``self.weights``.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        #\n",
    "        # 1. feedforward: calculate inputs and activities at each layer\n",
    "        #\n",
    "        activation = x    # this is the input layer (passed as argument)\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        \n",
    "        #\n",
    "        # 2. backward pass: calculate the ``error`` at each layer\n",
    "        #      by propagating errors back\n",
    "        #\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Note that the numeration of layers, here grows from the output\n",
    "        # to the input layer (Python convention for negative indices in\n",
    "        # lists)\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\n",
    "        \"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \"\"\"Return the number of inputs in ``data`` for which the neural\n",
    "        network outputs the correct result. The neural network's\n",
    "        output is assumed to be the index of whichever neuron in the\n",
    "        final layer has the highest activation.\n",
    "\n",
    "        The flag ``convert`` should be set to False if the data set is\n",
    "        validation or test data (the usual case), and to True if the\n",
    "        data set is the training data. The need for this flag arises\n",
    "        due to differences in the way the results ``y`` are\n",
    "        represented in the different data sets.  In particular, it\n",
    "        flags whether we need to convert between the different\n",
    "        representations.  It may seem strange to use different\n",
    "        representations for the different data sets.  Why not use the\n",
    "        same representation for all three data sets?  It's done for\n",
    "        efficiency reasons -- the program usually evaluates the cost\n",
    "        on the training data and the accuracy on other data sets.\n",
    "        These are different types of computations, and using different\n",
    "        representations speeds things up.  More details on the\n",
    "        representations can be found in\n",
    "        mnist_loader.load_data_wrapper.\n",
    "\n",
    "        \"\"\"\n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "\n",
    "        result_accuracy = sum(int(x == y) for (x, y) in results)\n",
    "        return result_accuracy\n",
    "    \n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Let's get to the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 282 / 1000\n",
      "Epoch 1 : 334 / 1000\n",
      "Epoch 2 : 402 / 1000\n",
      "Epoch 3 : 457 / 1000\n",
      "Epoch 4 : 539 / 1000\n",
      "Epoch 5 : 583 / 1000\n",
      "Epoch 6 : 620 / 1000\n",
      "Epoch 7 : 623 / 1000\n",
      "Epoch 8 : 629 / 1000\n",
      "Epoch 9 : 647 / 1000\n",
      "Epoch 10 : 651 / 1000\n",
      "Epoch 11 : 646 / 1000\n",
      "Epoch 12 : 651 / 1000\n",
      "Epoch 13 : 652 / 1000\n",
      "Epoch 14 : 652 / 1000\n",
      "Epoch 15 : 654 / 1000\n",
      "Epoch 16 : 652 / 1000\n",
      "Epoch 17 : 650 / 1000\n",
      "Epoch 18 : 646 / 1000\n",
      "Epoch 19 : 655 / 1000\n",
      "Epoch 20 : 651 / 1000\n",
      "Epoch 21 : 653 / 1000\n",
      "Epoch 22 : 646 / 1000\n",
      "Epoch 23 : 649 / 1000\n",
      "Epoch 24 : 651 / 1000\n",
      "Epoch 25 : 651 / 1000\n",
      "Epoch 26 : 650 / 1000\n",
      "Epoch 27 : 651 / 1000\n",
      "Epoch 28 : 653 / 1000\n",
      "Epoch 29 : 650 / 1000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "\n",
    "import network\n",
    "net = network.Network([784, 30, 10])\n",
    "\n",
    "epochs = 30\n",
    "mini_batch_size = 10\n",
    "eta = 3.0\n",
    "\n",
    "### uncomment to reduce the training data set\n",
    "n_training = 1000  # < 50000\n",
    "training_data = list(training_data)[:n_training]\n",
    "\n",
    "n_test = 1000\n",
    "test_data = list(test_data)[:n_test]\n",
    "\n",
    "training_accuracy, evaluation_accuracy = net.SGD(training_data, epochs, mini_batch_size, eta, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(0, 30, 1), training_accuracy, color='#FFA933',\n",
    "            label=\"Accuracy on the training data\")\n",
    "ax.plot(np.arange(0, 30, 1), evaluation_accuracy, color='#2A6EA6', \n",
    "            label=\"Accuracy on the test data\")\n",
    "ax.set_xlim(0, epochs)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Classification accuracy')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Explore the parameters of the network\n",
    "**?)** What is the performance when the number of layers is increased?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Change the structure of the output\n",
    "The code as it is constructs a neural network in which the last layer has 10 nodes, and the \"answer\" of the network for the classification of the input image is the index of the node which has maximum activity. \n",
    "\n",
    "**?)** How does the code change if we want as \"answer\" the binary representation of the number?\n",
    "\n",
    "**?)** How is the performance affected?\n",
    "\n",
    "*Practical hint:* one need to change the functions ``vectorized_result`` and ``accuracy``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
